out2 <- kmeans(df, centers = 2, iter.max = 15, nstart=5) # applying k-means with k = 2
out3 <- kmeans(df, centers = 3, iter.max = 15, nstart=5) # applying k-means with k = 3
out4 <- kmeans(df, centers = 4, iter.max = 15, nstart=5) # applying k-means with k = 4
# plotting elbow graph to select best k value and make use of the select k value
k.max <- 10 # max no of clusters to test
wss <- sapply(1:k.max,
function(k){kmeans(df, k, nstart=5,iter.max = 15 )$tot.withinss})
# computing WSS for k = 1 to 10 to identify elbow point
# plotting wss vs k to find the eblow point
plot(1:k.max, wss,
type="b", pch = 19, frame = FALSE,
xlab="Number of clusters K",
ylab="Total within-clusters sum of squares")
# ran k-means with optimal k = 3 (determined visually from elbow plot)
k_optimal <- kmeans(df, centers = 3, iter.max = 15, nstart = 5)
table(k_optimal$cluster) # size of each cluster for k= 3
# show total WSS for the final clustering
k_optimal$tot.withinss
# K-means clustering was applied with k=2, 3, and 4 to see how the data groups.The elbow method showed that k=3 is the best choice because the rate of WSS decrease slowed significantly beyond this point. Clustering with k=3 resulted in three groups of sizes 127, 103, and 170 and WSS of 39.810.
# Plot clusters
df$clusters <- as.factor(out3$cluster) #Assigns each data point to a cluster based on the k-means clustering result stored in out3
# and now scatter plot of gene1 vs gene2, coloring points by their cluster
ggplot(df, aes(x = gene1, y = gene2)) +
geom_point(aes(col = clusters)) +  # adding points and color them by cluster
# customized our plot appearance
theme(
panel.background = element_rect(fill = "white", colour = "black",
linewidth = 1, linetype = "solid"),  # let's set white background with black border
panel.grid.major = element_blank(),  # removing major grid lines
panel.grid.minor = element_blank(),  # and also minor grid lines
text = element_text(size = 20, colour = "black", face = "bold"),  # setting font size, color, and bold text
legend.key = element_blank()  # remove background from legend
)
# The scatter plot reveals three clusters based on gene1 and gene expression, with some overlaps among them and no sharp boundaries.
# computing distance matrix and applying hierarchical clustering with two linkage methods
dist_mat <- dist(df, method = 'euclidean') # computing Euclidean distance matrix for all samples
hclust_com<- hclust(dist_mat, method = 'complete') # performing hierarchical clustering with complete linkage
hclust_avg<- hclust(dist_mat, method = 'average') # performing hierarchical clustering with average linkage
plot(hclust_com, main = "Complete Linkage Dendrogram",
xlab = "Samples", ylab = "Distance", hang = -1, cex = 0.6) # plotting dendrogram for complete linkage
plot(hclust_avg, main = "Average Linkage Dendrogram",
xlab = "Samples", ylab = "Distance", hang = -1, cex = 0.6) # plotting dendrogram for average linkage
# Complete linkage produces more distinct and interpretable clusters as its dendrogram shows taller branches and higher merge distances (up to 3.0) which indicates better separation between clusters compared to average linkage. While average linkage has a more compressed structure with a maximum height of around 1.5.
plot(hclust_com, main = "Complete Linkage Dendrogram",
xlab = "Samples", ylab = "Distance", hang = -1, cex = 0.6) # plotting dendrogram for complete linkage
rect.hclust(hclust_com, k = 3, border = 2:4) # cut at k = 3
plot(hclust_avg, main = "Average Linkage Dendrogram",
xlab = "Samples", ylab = "Distance", hang = -1, cex = 0.6) # plotting dendrogram for average linkage
rect.hclust(hclust_avg, k = 3, border = 2:4) # cut at k = 3
# extracting cluster assignments from hierarchical clustering
com_clusters <- cutree(hclust_com, k = 3)  # Complete linkage clusters
avg_clusters <- cutree(hclust_avg, k = 3)  # Average linkage clusters
# extracting k-means cluster assignments (k = 3, from Q2)
kmeans_clusters <- kmeans(df, centers = 3, iter.max = 15, nstart = 5)$cluster
table(com_clusters, kmeans_clusters) # comparing complete linkage with k-means
# let's compare average linkage with k-means
table(avg_clusters, kmeans_clusters) # comparing average linkage with k-means
# I cut both dendrograms at k = 3 to match the optimal k from k-means, determined by the elbow method in Q2.
# The contingency table for complete linkage vs. k-means shows moderate agreement. In contrast, the average linkage vs. k-means table shows perfect agreement. The average linkage cluster 1 matches k-means cluster 1 (127 samples), cluster 2 matches k-means cluster 3 (170 samples), and cluster 3 matches k-means cluster 2 (103 samples). The close match between average linkage and k-means is unexpected since average linkage had a more compressed dendrogram in Q5.
df <- read.csv('Labelled_data_Student4.csv', row.names = 1) ## loaded my file
# we are now filtering dataset for class 'c1' and 'c2' as well as converting class col into a factor
df_filtered <- df %>%
filter(class %in% c("c1", "c2")) %>%
mutate(class = factor(class, levels = c("c1", "c2")))
#dim(df_filtered) # only 58 rows and the same no of cols 2371
set.seed(122) # let's set the seed to ensure reproducible train-test split
# come on now let's split the filtered data into 70% training and 30% testing
trainIndex <- createDataPartition(df_filtered$class, p = 0.7, list = FALSE, times=1)
df_train <- df_filtered[trainIndex, ]
df_test <- df_filtered[-trainIndex, ]
#dim(df_train) # 41 2371
#dim(df_test) # 17 2371
#table(df_train$class)  # Class distribution in training c1: 13, c2: 28
#table(df_test$class)   # Class distribution in testing  c1: 5, c2: 12
p_val <- sapply(1:2370, function(i) {
t.test(df_train[[i]] ~ df_train$class)$p.value
}) # performing t-test for each metabolite to find the most significant predictor
best_predictor_idx <- which.min(p_val) #getting index of smallest p value
# we selected the metabolite with the lowest p-value since it shows the strongest statistical difference between c1 and c2.
best_predictor <- names(df_train)[best_predictor_idx] # getting the name of metabolite with least p value/most significant
cat("Chosen predictor:", best_predictor, "with p-value:", p_val[best_predictor_idx], "\n")
set.seed(122)
model1 <- glm(as.formula(paste("class ~", best_predictor)), data = df_train[, c(best_predictor, "class")], family = "binomial")  # training the model
# get summary of the model
summary(model1) # The logistic regression model’s coefficient (2.679e-06, p = 0.00755) suggests a small, positive and statistically significant effect in predicting class c2.
pred <- predict(model1, newdata=df_test, type='response') # using model1 to predict probabilities for the test data
pred[pred>=0.5] <- "c2" # if probabilities are at least 50% then they are considered as c2
pred[pred<0.5] <- "c1" # or else as c1
pred <- as.factor(pred) # converting the ch to factor
CM <- confusionMatrix(pred,df_test$class) # comparing predictions with the true classes in test data
CM # print the confusion matrix and its metrics
cat("Accuracy:", CM$overall["Accuracy"], "\n") # Accuracy of the model: 64.70%
cat("Sensitivity:", CM$byClass["Sensitivity"], "\n") # Sensitivity: 20% # how well the model identifies the “positive” class (here, c1 by default)
cat("Specificity:", CM$byClass["Specificity"], "\n") # Specificity: 83.33% # how well the model identifies the “negative” class (here, c2 ).
# The logistic regression model shows strong specificity, but its low sensitivity and moderate accuracy suggest it struggles to correctly identify c1 and tends to predict c2 more often.
# Set seed for reproducibility
set.seed(122)
# df_filtered is defined from Q7:
# Define 7-fold cross-validation using fold() from groupdata2, stratified by 'class'
folds <- fold(data = df_filtered, k = 7, cat_col = "class")$`.folds`
# Splits the data into 7 parts (folds), making sure each part has a similar mix of "c1" and "c2". Stores which rows go in each fold.
# Creating a table with 7 rows (one for each fold) to store Accuracy, Sensitivity, and Specificity scores (starts empty with NA).
results <- data.frame(Fold = 1:7, Accuracy = NA, Sensitivity = NA, Specificity = NA)
# Perform 7-fold cross-validation
for (k in 1:7) {
# splitting data - one part to test, the rest to train on.
df_test <- df_filtered[folds == k, ] # Pick rows for the current fold as test data
df_train <- df_filtered[folds != k, ] # Use all other rows as training data
# Train logistic regression model with the best predictor from Q8
# best_predictor was defined from Q8:
formula <- as.formula(paste("class ~", best_predictor))
model <- glm(formula, data = df_train[, c(best_predictor, "class")], family = "binomial")
pred <- predict(model, newdata = df_test, type = "response") # get probability scores for test data
pred[pred >= 0.5] <- "c2"  # if score is 50% or more, call it "c2"
pred[pred < 0.5] <- "c1"   # if less than 50%, call it "c1"
pred <- factor(pred, levels = c("c1", "c2"))  # converting to factor
# let's compute confusion matrix
CM <- confusionMatrix(data = pred, reference = df_test$class)
# Store specific metrics
results$Accuracy[k] <- CM$overall["Accuracy"]
results$Sensitivity[k] <- CM$byClass["Sensitivity"]
results$Specificity[k] <- CM$byClass["Specificity"]
}
# Print results
print(results)
# Plot boxplots
par(mfrow = c(1, 3))  # Arrange 3 plots in 1 row
boxplot(results$Accuracy, main = "Accuracy Across Folds", ylab = "Accuracy", ylim = c(0, 1))
boxplot(results$Sensitivity, main = "Sensitivity Across Folds", ylab = "Sensitivity", ylim = c(0, 1))
boxplot(results$Specificity, main = "Specificity Across Folds", ylab = "Specificity", ylim = c(0, 1))
# Boxplots show variability in performance across folds, with specificity generally higher than sensitivity.
df <- read.csv('Labelled_data_Student4.csv', row.names = 1) ## loaded my file
#dim(df) # so intially there are 100 rows and 2371 cols
set.seed(122)
df_filtered <- df %>% filter(class %in% c("c1", "c2")) %>% mutate(class = factor(class, levels = c("c1", "c2"))) # now only 58 rows and same no of cols (2371)
trainIndex <- createDataPartition(df_filtered$class, p = 0.7, list = FALSE, times = 1)
df_train <- df_filtered[trainIndex, ] # 41 rows 2371 cols
df_test <- df_filtered[-trainIndex, ] # 17 rows 2371 rows
# Train the LDA model on df_train using all predictors (except class)
lda_model <- lda(class ~ ., data = df_train)
# What it does: Builds an LDA model to predict "class" using all columns in df_train except "class"
# Print the model to see what it learned
print(lda_model)
#extracting LD1 scores from LDA model for training data to project into reduced space
lda_scores <- predict(lda_model, df_train)$x
# creating a data frame combining LD1 scores with class labels for plotting
plot_data <- data.frame(LD1 = lda_scores[, 1], Class = df_train$class)
# Scatter plot to show separation of classes
plot(plot_data$LD1, col = ifelse(plot_data$Class == "c1", "blue", "red"),
pch = 16, xlab = "LD1 Score", ylab = "", main = "LDA Reduced Space",
yaxt = "n")
legend("topright", legend = c("c1", "c2"), col = c("blue", "red"), pch = 16)
# The scatter plot shows the training data projected onto the first linear discriminant (LD1) where blue dots are class c1 and red dots are class c2.Most c1 points are grouped at higher LD1 scores (between 30 and 45), while c2 points are more scattered across lower scores (between 0 and 35).However, there is significant overlap between the classes in the 15–35 range which shows that LDA can only partly separate the two classes.
# predict classes for df_test using the trained LDA model
lda_pred <- predict(lda_model, df_test)
# extracting predicted and true classes
predicted_classes <- lda_pred$class
true_classes <- df_test$class
# computing confusion matrix comparing true vs. predicted classes
conf_matrix <- table(True = true_classes, Predicted = predicted_classes)
# extracted counts from confusion matrix, treating c1 as the positive class
TP <- conf_matrix[1, 1]  # True Positives (c1 predicted as c1)
FN <- conf_matrix[1, 2]  # False Negatives (c1 predicted as c2)
FP <- conf_matrix[2, 1]  # False Positives (c2 predicted as c1)
TN <- conf_matrix[2, 2]  # True Negatives (c2 predicted as c2)
# Accuracy
accuracy <- (TP + TN) / sum(conf_matrix)
# Sensitivity (for c1)
sensitivity <- TP / (TP + FN)
# Specificity (for c1)
specificity <- TN / (TN + FP)
# Printing results
cat("Confusion Matrix:\n")
print(conf_matrix)
cat("\nAccuracy:", accuracy, "\n")
cat("Sensitivity (c1):", sensitivity, "\n")
cat("Specificity (c1):", specificity, "\n")
# The LDA model has accuracy of 76.47%, with a high specificity of 91.67% but a lower sensitivity of 40% for class c1. This indicates that the model is better at correctly identifying c2 samples than c1 samples. This is because of the overlap in LD1 scores seen in Q12 and the class imbalance (more c2 samples).
# set seed for reproducibility
set.seed(122)
# df_filtered is defined from Q7 (58 rows: 18 c1, 40 c2)
# define 7-fold cross-validation using fold() from groupdata2, stratified by 'class'
folds <- fold(data = df_filtered, k = 7, cat_col = "class")$`.folds`
# Splits the 58 rows into 7 folds (~8-9 rows per fold), ensuring each fold has a similar proportion of c1 and c2
# create a table to store metrics for each fold
results <- data.frame(Fold = 1:7, Accuracy = NA, Sensitivity = NA, Specificity = NA)
# perform 7-fold cross-validation
for (k in 1:7) {
# split data: one fold for testing, the rest for training to evaluate model performance
df_test_cv <- df_filtered[folds == k, ]  # Test fold
df_train_cv <- df_filtered[folds != k, ] # Training folds
# train LDA model on the training fold using all predictors
lda_model_cv <- lda(class ~ ., data = df_train_cv)
# predict classes for the test fold
lda_pred <- predict(lda_model_cv, df_test_cv)
pred_classes <- lda_pred$class
# compute confusion matrix using caret, with c1 as the positive class
CM <- confusionMatrix(data = pred_classes, reference = df_test_cv$class, positive = "c1")
# Store metrics from confusion matrix for later visualization
results$Accuracy[k] <- CM$overall["Accuracy"]
results$Sensitivity[k] <- CM$byClass["Sensitivity"]
results$Specificity[k] <- CM$byClass["Specificity"]
}
# print results table
print(results)
# plot boxplots to visualize metric variability across folds
par(mfrow = c(1, 3))  # Arrange 3 plots in 1 row
boxplot(results$Accuracy, main = "Accuracy Across Folds", ylab = "Accuracy", ylim = c(0, 1))
boxplot(results$Sensitivity, main = "Sensitivity Across Folds", ylab = "Sensitivity", ylim = c(0, 1))
boxplot(results$Specificity, main = "Specificity Across Folds", ylab = "Specificity", ylim = c(0, 1))
# The boxplots show how LDA’s performance changes across the 7 folds, with accuracy ranging from 55.6% to 88.9%, sensitivity for c1 fluctuating widely (33.3% to 100%), and specificity for c1 generally higher (50% to 100%). It’s better at spotting c2 than c1
library(e1071) # loading the necessary library for SVM
df <- read.csv('Labelled_data_Student4.csv', row.names = 1) # loaded our data
df_filtered <- df %>% filter(class %in% c("c1", "c2")) %>% mutate(class = factor(class, levels = c("c1", "c2"))) # taking into account only class 1 and 2 and turning them into factor
set.seed(122) # setting for reproducibility
trainIndex <- createDataPartition(df_filtered$class, p = 0.7, list = FALSE, times = 1)
df_train <- df_filtered[trainIndex, ]
df_test <- df_filtered[-trainIndex, ]
# checking df_train dimensions and class distribution
#dim(df_train)  # has 41 rows and 2371 columns
#table(df_train$class)  # has 13 c1, 28 c2
# training the SVM model on df_train
svm_model <- svm(class ~ ., data = df_train, kernel = "linear", scale = TRUE) # linear kernel chosen for simplicity and scale = TRUE standardizes features.
# printing the model summary
print(svm_model) # SVM with a linear kernel uses 30 support vectors, indicating moderate complexity in separating c1 and c2.
# predict classes for df_test using the SVM model
svm_pred <- predict(svm_model, df_test)
# true classes from df_test
true_classes <- df_test$class
# compute confusion matrix to compare true vs. predicted classes
conf_matrix <- table(True = true_classes, Predicted = svm_pred)
# extract counts for metrics, treating c1 as the positive class
TP <- conf_matrix[1, 1]  # True Positives (c1 predicted as c1)
FN <- conf_matrix[1, 2]  # False Negatives (c1 predicted as c2)
FP <- conf_matrix[2, 1]  # False Positives (c2 predicted as c1)
TN <- conf_matrix[2, 2]  # True Negatives (c2 predicted as c2)
# Accuracy
accuracy <- (TP + TN) / sum(conf_matrix)
# Sensitivity (for c1)
sensitivity <- TP / (TP + FN)
# Specificity (for c1)
specificity <- TN / (TN + FP)
# Print results
cat("Confusion Matrix:\n")
print(conf_matrix)
cat("\nAccuracy:", accuracy, "\n") # 64.71% accuracy
cat("Sensitivity (c1):", sensitivity, "\n") # 60% Sensitivity
cat("Specificity (c1):", specificity, "\n") # 66.66% Specificity
# df_filtered is already defined in Q7, it has (58 rows: 18 c1, 40 c2, 2371 columns)
# setting seed for reproducibility
set.seed(122) # setting for reproducibility
# created 7 folds, stratified by class
folds <- fold(data = df_filtered, k = 7, cat_col = "class")$`.folds`
# created a table to store metrics for each fold
results <- data.frame(Fold = 1:7, Accuracy = NA, Sensitivity = NA, Specificity = NA)
# performing 7-fold cross-validation
for (k in 1:7) {
# split data: one fold for testing, the rest for training
df_test_cv <- df_filtered[folds == k, ]
df_train_cv <- df_filtered[folds != k, ]
# training SVM model on the training fold
svm_model_cv <- svm(class ~ ., data = df_train_cv, kernel = "linear", scale = TRUE)
# Predict classes for the test fold
svm_pred <- predict(svm_model_cv, df_test_cv)
# compute confusion matrix to evalulate performance
conf_matrix <- table(True = df_test_cv$class, Predicted = svm_pred)
# extract counts
TP <- conf_matrix[1, 1]  # True Positives (c1 predicted as c1)
FN <- conf_matrix[1, 2]  # False Negatives (c1 predicted as c2)
FP <- conf_matrix[2, 1]  # False Positives (c2 predicted as c1)
TN <- conf_matrix[2, 2]  # True Negatives (c2 predicted as c2)
# Compute metrics
results$Accuracy[k] <- (TP + TN) / sum(conf_matrix)
results$Sensitivity[k] <- TP / (TP + FN)
results$Specificity[k] <- TN / (TN + FP)
}
# Print the results table
print(results)
# Plot boxplots for Accuracy, Sensitivity, and Specificity
par(mfrow = c(1, 3))  # Arrange 3 plots in 1 row
boxplot(results$Accuracy, main = "Accuracy Across Folds", ylab = "Accuracy", ylim = c(0, 1))
boxplot(results$Sensitivity, main = "Sensitivity Across Folds", ylab = "Sensitivity", ylim = c(0, 1))
boxplot(results$Specificity, main = "Specificity Across Folds", ylab = "Specificity", ylim = c(0, 1))
# boxplots show SVM performance across folds, with accuracy (62.5%–88.9%) and specificity (71.4%–100%) generally high, but sensitivity (25%–100%) varies widely due to few c1 samples.
setwd("C:/Users/Nikita Maurya/Downloads/archive")
library(readxl)
Clinical_Data_Validation_Cohort <- read_excel("Clinical_Data_Validation_Cohort.xlsx")
View(Clinical_Data_Validation_Cohort)
source("~/.active-rstudio-document", echo=TRUE)
summary(data)
data$Event = as.factor(data$Event)
data$Event = as.factor(data$`Event (death: 1, alive: 0)`)
data$Grade <- as.factor(data$Grade)
data$Stage <- as.factor(data$`Stage (TNM 8th edition)`)
data$Sex <- as.factor(data$Sex)
data$Cigarette <- as.factor(data$Cigarette)
data$Type.Adjuvant <- as.factor(data$Type.Adjuvant)
data$EGFR <- as.factor(data$EGFR)
data$KRAS <- as.factor(data$KRAS)
library(dplyr)
source("~/.active-rstudio-document", echo=TRUE)
summary(data)
summary(data)
source("C:/Users/Nikita Maurya/Downloads/archive/cleandata.R", echo=TRUE)
source("C:/Users/Nikita Maurya/Downloads/archive/cleandata.R", echo=TRUE)
dim(data)
library(survival)
library(survminer)
library(survival)
library(survminer)
s = Surv(data$`Survival time (days)`, data$Event)
s
sfit = survfit(Surv('Survival time (days)', 'Event')~1, data)
summary(data$`Survival time (days)`)
summary(data$Event)
sfit = survfit(Surv('Survival time (days)', Event)~1, data)
sfit = survfit(Surv(`Survival time (days)`, Event) ~ 1, data = data)
sfit
summary(sfit)
plot(sfit)
ggsurvplot(sfit)
library(survminer)
ggsurvplot(sfit)
library(survminer)
ggsurvplot(
sfit,
data = data,
conf.int = TRUE,         # show 95% confidence interval
risk.table = TRUE,       # show number at risk below plot
xlab = "Days",
ylab = "Survival Probability",
title = "Overall Survival Curve"
)
source("C:/Users/Nikita Maurya/Downloads/archive/cleandata.R", echo=TRUE)
library(survminer)
ggsurvplot(
sfit,
data = data,
conf.int = TRUE,         # show 95% confidence interval
risk.table = TRUE,       # show number at risk below plot
xlab = "Days",
ylab = "Survival Probability",
title = "Overall Survival Curve"
)
summary(sfit)
source("C:/Users/Nikita Maurya/Downloads/archive/cleandata.R", echo=TRUE)
source("C:/Users/Nikita Maurya/Downloads/archive/cleandata.R", echo=TRUE)
sfit2 = coxph(Surv(time, status)~sex, data = lung)
summary(sfit2)
sfit2 = coxph(Surv(time, status)~sex, data)
sfit2 = coxph(Surv(`Survival time (days)`, Event) ~ Sex, data = data)
summary(sfit2)
source("C:/Users/Nikita Maurya/Downloads/archive/cleandata.R", echo=TRUE)
source("C:/Users/Nikita Maurya/Downloads/archive/cleandata.R", echo=TRUE)
source("C:/Users/Nikita Maurya/Downloads/archive/cleandata.R", echo=TRUE)
sfit3 = survfit(Surv(`Survival time (days)`, Event) ~ Type.Adjuvant, data = data)
summary(sfit3)
ggsurvplot(
sfit3,
data = data,
risk.table = TRUE,
xlab = "Days",
ylab = "Survival Probability",
title = "Overall Survival Curve"
)
sfit3_adjuvant = coxph(Surv(`Survival time (days)`, Event) ~ Type.Adjuvant, data = data)
summary(sfit3_adjuvant)
source("C:/Users/Nikita Maurya/Downloads/archive/cleandata.R", echo=TRUE)
sfit_summary <- summary(sfit)
sfit_df <- data.frame(
time = sfit_summary$time,
n.risk = sfit_summary$n.risk,
n.event = sfit_summary$n.event,
survival = sfit_summary$surv,
lower95 = sfit_summary$lower,
upper95 = sfit_summary$upper
)
View(sfit_df)
write.csv(sfit_df, "overall_survival.csv", row.names = FALSE)
sfit_summary1 <- summary(sfit1)
sfit_df1 <- data.frame(
time = sfit_summary1$time,
n.risk = sfit_summary1$n.risk,
n.event = sfit_summary1$n.event,
survival = sfit_summary1$surv,
lower95 = sfit_summary1$lower,
upper95 = sfit_summary1$upper
)
write.csv(sfit_df1, "overall_survival1.csv", row.names = FALSE)
sfit_summary2 <- summary(sfit2)
sfit_df2 <- data.frame(
time = sfit_summary2$time,
n.risk = sfit_summary2$n.risk,
n.event = sfit_summary2$n.event,
survival = sfit_summary2$surv,
lower95 = sfit_summary2$lower,
upper95 = sfit_summary2$upper
)
write.csv(sfit_df2, "overall_survival2.csv", row.names = FALSE)
sfit_summary3 <- summary(sfit3)
sfit_df3 <- data.frame(
time = sfit_summary3$time,
n.risk = sfit_summary3$n.risk,
n.event = sfit_summary3$n.event,
survival = sfit_summary3$surv,
lower95 = sfit_summary3$lower,
upper95 = sfit_summary3$upper
)
write.csv(sfit_df3, "overall_survival.csv", row.names = FALSE)
View(sfit_df1)
View(sfit_df2)
View(sfit_df3)
sfit_summary3 <- summary(sfit3)
sfit_df3 <- data.frame(
time = sfit_summary3$time,
n.risk = sfit_summary3$n.risk,
n.event = sfit_summary3$n.event,
survival = sfit_summary3$surv,
lower95 = sfit_summary3$lower,
upper95 = sfit_summary3$upper
)
write.csv(sfit_df3, "overall_survival3.csv", row.names = FALSE)
sfit_summary1 <- summary(sfit1)
sfit_summary1
summary(data)
dir()
p1 <- ggsurvplot(sfit, data = data, risk.table = TRUE, title = "Overall Survival")$plot
p2 <- ggsurvplot(sfit1, data = data, risk.table = TRUE, title = "Survival by Sex")$plot
p3 <- ggsurvplot(sfit2, data = data, risk.table = TRUE, title = "Survival by Grade")$plot
p4 <- ggsurvplot(sfit3, data = data, risk.table = TRUE, title = "Survival by Therapy")$plot
# Combine all four plots into one figure (2x2 grid)
combined_plot <- (p1 | p2) / (p3 | p4)
p1 <- ggsurvplot(sfit, data = data, risk.table = TRUE, title = "Overall Survival")$plot
p2 <- ggsurvplot(sfit1, data = data, risk.table = TRUE, title = "Survival by Sex")$plot
p3 <- ggsurvplot(sfit2, data = data, risk.table = TRUE, title = "Survival by Grade")$plot
p4 <- ggsurvplot(sfit3, data = data, risk.table = TRUE, title = "Survival by Therapy")$plot
library(patchwork)
combined_plot <- (p1 | p2) / (p3 | p4)
combined_plot
# Optionally save as PNG
ggsave("combined_survival_plots.png", combined_plot, width = 14, height = 10)
# Optionally save as PNG
ggsave("combined_survival_plots.png", combined_plot, width = 16, height = 10)
ggsave("combined_survival_plots.png", combined_plot, width = 12, height = 10)
source("C:/Users/Nikita Maurya/Downloads/archive/cleandata.R", echo=TRUE)
source("C:/Users/Nikita Maurya/Downloads/archive/cleandata.R", echo=TRUE)
source("C:/Users/Nikita Maurya/Downloads/archive/cleandata.R", echo=TRUE)
source("C:/Users/Nikita Maurya/Downloads/archive/cleandata.R", echo=TRUE)
p
p1
p = ggsurvplot(
sfit,
data = data,
xlab = "Days",
ylab = "Survival Probability",
title = "Overall Survival Curve"
)
p1 = ggsurvplot(
sfit1,
data = data,
xlab = "Days",
ylab = "Survival Probability By Sex",
title = "Survival Curve"
)
p2 = ggsurvplot(
sfit2,
data = data,
xlab = "Days",
ylab = "Survival Probability By Grade",
title = "Overall Survival Curve"
)
p3 = ggsurvplot(
sfit3,
data = data,
xlab = "Days",
ylab = "Survival Probability By Therapy",
title = "Overall Survival Curve"
)
p
p1
p2
p3
p
p1
p1 = ggsurvplot(
sfit1,
data = data,
xlab = "Days",
ylab = "Survival Probability By Sex",
title = "Survival Curve"
)
p1
p1
source("C:/Users/Nikita Maurya/Downloads/archive/cleandata.R", echo=TRUE)
source("C:/Users/Nikita Maurya/Downloads/archive/cleandata.R", echo=TRUE)
source("C:/Users/Nikita Maurya/Downloads/archive/cleandata.R", echo=TRUE)
# Save p
ggsave("Overall_Survival.png", plot = p$plot, width = 7, height = 5, dpi = 300)
# Save p1
ggsave("Survival_by_Sex.png", plot = p1$plot, width = 7, height = 5, dpi = 300)
# Save p3
ggsave("Survival_by_Grade.png", plot = p3$plot, width = 7, height = 5, dpi = 300)
# Save p4
ggsave("Survival_by_Therapy.png", plot = p4$plot, width = 7, height = 5, dpi = 300)
# Save p
ggsave("Overall_Survival.png", plot = p$plot, width = 7, height = 5, dpi = 300)
# Save p1
ggsave("Survival_by_Sex.png", plot = p1$plot, width = 7, height = 5, dpi = 300)
# Save p2
ggsave("Survival_by_Grade.png", plot = p2$plot, width = 7, height = 5, dpi = 300)
# Save p3
ggsave("Survival_by_Therapy.png", plot = p3$plot, width = 7, height = 5, dpi = 300)
